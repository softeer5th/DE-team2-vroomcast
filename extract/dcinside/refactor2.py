from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from datetime import datetime
from dateutil import parser
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
import time, json, logging, requests, os
from bs4 import BeautifulSoup
import boto3, random

logging.basicConfig(level=logging.INFO)  # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
logger = logging.getLogger(__name__)


BASE_URL = "https://gall.dcinside.com/board/lists/?id=car_new1"
WAIT_TIME = 2
# ì œëª©ë§Œ / ì œëª©+ë‚´ìš©
SEARCH_URL_TITLE = f"https://gall.dcinside.com/board/lists/?id=car_new1&s_type=search_subject&s_keyword="
SEARCH_URL_TITLE_AND_CONTENT = f"https://gall.dcinside.com/board/lists/?id=car_new1&s_type=search_subject_memo&s_keyword="  

def convert_date_format(date_str):
    # date_str = str(md_to_ymd(date_str))
    """
    Converts a date string to a standardized date format.
    
    Extracts the date portion (ignoring any time component) from the input string and
    replaces periods ('.') with hyphens ('-'). For example, "2024.10.30 12:00:00" becomes "2024-10-30".
    
    Args:
        date_str (str): The input date string, potentially including time data.
    
    Returns:
        str: The date portion of the input string with periods replaced by hyphens.
    """
    return str(date_str).split(' ')[0].replace('.', '-')

def md_to_ymd(date_str:str):
    """
    ì…ë ¥ëœ ë‚ ì§œ ë¬¸ìì—´ì„ "yyyy.mm.dd HH:MM:SS" í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ì§€ì›í•˜ëŠ” ë‚ ì§œ í˜•ì‹:
      - "yyyy.mm.dd HH:MM:SS": ë³€í™˜ ì—†ì´ ì›ë³¸ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
      - "mm.dd HH:MM:SS": í˜„ì¬ ì—°ë„ë¥¼ ì ìš©í•˜ì—¬ "yyyy.mm.dd HH:MM:SS" í˜•ì‹ìœ¼ë¡œ ë³€í™˜.
    
    Args:
        date_str (str): ë³€í™˜í•  ë‚ ì§œ ë¬¸ìì—´. ì§€ì› í˜•ì‹ì€ "yyyy.mm.dd HH:MM:SS" ë˜ëŠ” "mm.dd HH:MM:SS"ì…ë‹ˆë‹¤.
    
    Returns:
        str: "yyyy.mm.dd HH:MM:SS" í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´. ì…ë ¥ í˜•ì‹ì´ ì§€ì›ë˜ì§€ ì•Šì„ ê²½ìš° "Invalid date format"ë¥¼ ë°˜í™˜.
    
    Examples:
        >>> md_to_ymd("2024.12.31 23:59:59")
        "2024.12.31 23:59:59"
        >>> md_to_ymd("12.31 23:59:59")
        "2025.12.31 23:59:59"
    """
    try:
        # "yyyy.mm.dd HH:MM:SS" í˜•ì‹ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        datetime.strptime(date_str, "%Y.%m.%d %H:%M:%S")
        return date_str
    except ValueError:
        try:
            # "mm.dd HH:MM:SS" í˜•ì‹ì¸ ê²½ìš° ì—°ë„ë¥¼ 2025ë¡œ ê°€ì •í•˜ì—¬ ë³€í™˜
            date_obj = datetime.strptime(date_str, "%m.%d %H:%M:%S")
            return date_obj.replace(year=datetime.now().year).strftime("%Y.%m.%d %H:%M:%S")
        except ValueError:
            return "Invalid date format"

def date_type_for_search_result(date_str):
    """
    Parse a date string into a datetime object using supported formats.
    
    Attempts to interpret the input as "mm.dd" (with the current year) or "yy.mm.dd".
    Returns a datetime object if the string matches one of these formats; otherwise,
    returns the string "Invalid Date Format".
    
    Args:
        date_str (str): Date string in "mm.dd" or "yy.mm.dd" format.
    
    Returns:
        datetime.datetime or str: A datetime object if parsing is successful,
        or "Invalid Date Format" if neither format is matched.
    """
    try:
        # 'mm.dd' í˜•ì‹ ì²˜ë¦¬
        date_obj = datetime.strptime(date_str, "%m.%d")
        date_obj = date_obj.replace(year=datetime.now().year)
        # print(date_obj)
        return date_obj
    except ValueError:
        try:
            # 'yy.mm.dd' í˜•ì‹ ì²˜ë¦¬
            date_obj = datetime.strptime(date_str, "%y.%m.%d")
            # print(date_obj)
            return(date_obj)
        except ValueError:
            return "Invalid Date Format"

def is_date_in_range(date, start_date_str, end_date_str):
    """
    Verifies if a given date falls within the specified range.
    
    Parses the start and end dates from strings formatted as "YYYY-MM-DD" and
    checks whether the provided date (a datetime object) is between these dates,
    inclusive.
    
    Args:
        date (datetime.datetime): Date to validate.
        start_date_str (str): Start date as a string in "YYYY-MM-DD" format.
        end_date_str (str): End date as a string in "YYYY-MM-DD" format.
    
    Returns:
        bool: True if the date is within the range; False if it is outside the range.
        str: "Invalid Date Format" if either start_date_str or end_date_str cannot be parsed.
    """


    try:
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")

        return start_date <= date <= end_date

    except ValueError:
        return "Invalid Date Format"
    
def is_time_in_range(time_str, batch_time):
  """
  Checks whether a given time is within the range from six hours before to the reference time.
  
  Args:
      time_str (str): A time string in the format "%Y-%m-%d %H:%M:%S" representing the time to check.
      batch_time (str): A reference time string in the format "%Y-%m-%d %H:%M:%S" used as the upper bound.
  
  Returns:
      bool: True if time_str is between (batch_time - 6 hours) and batch_time, False otherwise.
  """

  try:
    input_time = datetime.datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
  except ValueError:
    return False  # ì˜ëª»ëœ í˜•ì‹ì˜ ë¬¸ìì—´

  now = datetime.datetime.strptime(batch_time, "%Y-%m-%d %H:%M:%S")
  six_hours_ago = now - datetime.timedelta(hours=6)
  print(now)
  print(input_time, "<-- Input time")
  print(six_hours_ago)

  return six_hours_ago <= input_time <= now    
class DC_crawler:
    MAX_TRY = 2
    RETRY_WAITS = 2
    post_link = [
    ]
    
    def __init__(self, s_date, e_date, car_id, car_keyword, is_daily_batch):
        """
        Initializes the DC_crawler instance with search parameters.
        
        Args:
            s_date (str): The desired start date for crawling (e.g., "yyyy-mm-dd").
            e_date (str): The desired end date for crawling (e.g., "yyyy-mm-dd").
            car_id (str): Identifier for the target car.
            car_keyword (str): Keyword related to the car to build the search URL.
            is_daily_batch (bool): If True, overrides s_date and e_date to the current date.
        
        Side Effects:
            Constructs the search URL by concatenating a global base URL with the car_keyword.
            When is_daily_batch is True, sets both start_date and end_date to the current date.
        """
        self.start_date = s_date
        self.end_date = e_date
        self.car_id = car_id
        self.keyword = car_keyword
        self.search_url = SEARCH_URL_TITLE + car_keyword
        if is_daily_batch: 
            self.daily_batch = datetime.now().strftime("%Y-%m-%d")
            self.start_date = daily_batch
            self.end_date = daily_batch
    # Chrome WebDriver ì„ ì–¸, Lambda ì ìš© ì‹œ ì£¼ì„ í•„íˆ ë³´ê³  í•´ì œí•  ê²ƒ!!!!!
    def _get_driver(self,):
        # ì´ pathëŠ” ë¡œì»¬ ì‹¤í–‰ ì‹œ ì£¼ì„ì²˜ë¦¬ í•˜ì„¸ìš”.
        # chrome_path = "/opt/chrome/chrome-headless-shell-mac-arm64"
        # driver_path = "/opt/chromedriver"   

        """
        Configures and returns a headless Chrome WebDriver with custom options.
        
        Sets up a Selenium Chrome WebDriver using ChromeOptions for headless operation, no sandbox,
        disabled GPU, a specific user-agent, and a fixed window size. Commented sections indicate alternative
        configurations for local execution.
            
        Returns:
            webdriver.Chrome: A headless Chrome WebDriver instance initialized with the specified options.
        """
        options = webdriver.ChromeOptions()
        # options.binary_location = chrome_path  # Chrome ì‹¤í–‰ íŒŒì¼ ì§€ì • (ë¡œì»¬ ì‹¤í–‰ ì‹œ ì£¼ì„ ì²˜ë¦¬)
        options.add_argument("--headless")  # Headless ëª¨ë“œ
        options.add_argument("--no-sandbox")
        # options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:135.0) Gecko/20100101 Firefox/135.0")
        # options.add_argument("user-agent=Mozilla/5.0 (compatible; Daum/3.0; +http://cs.daum.net/)")
        options.add_argument("--window-size=1920x1080")
        
        # service = Service(executable_path="/opt/chromedriver")
        driver = webdriver.Chrome(
            # service=service, # ë¡œì»¬ ì‹¤í–‰ ì‹œ ì£¼ì„ ì²˜ë¦¬
            options=options) 
        return driver
    
    def get_entry_point(self, driver:webdriver.Chrome, url):
        """
        Navigates to the target URL and applies a date filter to load search results.
        
        Opens the webpage provided by the URL, clicks the date picker button, and waits
        for its appearance. Sets the search date using the crawler's end_date attribute,
        submits the date using JavaScript and keystrokes, and then clicks the search
        button. After a short wait for results to load, returns the URL of the current
        page containing the search results.
        
        Args:
            driver (webdriver.Chrome): Selenium Chrome WebDriver instance.
            url (str): The URL to navigate to for initiating the date filter search.
        
        Returns:
            str: The URL of the loaded page after executing the search action.
        
        Raises:
            TimeoutException: If essential elements (date picker, date input, or search
                              button) are not found within the specified wait durations.
        """
        s_date = self.start_date
        e_date = self.end_date
        
        driver.get(url)
        time.sleep(WAIT_TIME)
        #-----------------------------------------------
        # ğŸ”¹ 1. ë‚ ì§œ ê²€ìƒ‰ ì°½ì„ ì—¬ëŠ” ë²„íŠ¼ í´ë¦­
        #-----------------------------------------------
        open_date_picker = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "button.btn_grey_roundbg.btn_schmove")))
        open_date_picker.click()
        time.sleep(1)  # ê²€ìƒ‰ ì°½ì´ ëœ¨ëŠ” ì‹œê°„ ê³ ë ¤
        
        #-----------------------------------------------
        # ğŸ”¹ 2. ë‚ ì§œ ì…ë ¥ í•„ë“œ ì°¾ê¸°
        #-----------------------------------------------
        date_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "input.dayin.calendar")))
        
        #-----------------------------------------------
        # ğŸ”¹ 3. ë‚ ì§œ ì…ë ¥
        #-----------------------------------------------
        target_date = e_date  # ê²€ìƒ‰í•  ë‚ ì§œ
        # JavaScriptë¡œ ë‚ ì§œ ê°’ ë³€ê²½
        driver.execute_script("arguments[0].value = arguments[1];", date_input, target_date)
        date_input.send_keys(target_date)
        date_input.send_keys(Keys.RETURN)  # ì—”í„° ì…ë ¥

        #-----------------------------------------------
        # ğŸ”¹ 4. ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
        #-----------------------------------------------
        search_btn = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "button.btn_blue.small.fast_move_btn"))
        )
        search_btn.click()

        #-----------------------------------------------
        # ğŸ”¹ 5. ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°
        #-----------------------------------------------
        time.sleep(0.5)  # ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì— ë”°ë¼ ì¡°ì •
        
        #-----------------------------------------------
        # ğŸ”¹ 6. í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        #-----------------------------------------------
        current_page_url = driver.current_url
        return current_page_url        
        
    def crawl_post_link(self, soup:BeautifulSoup, cur_date:str):
        """
        Extracts and processes post links from the current page's HTML.
        
        Scans the provided BeautifulSoup object for post entries identified by the "tr.ub-content.us-post" selector.
        For each post, the method extracts the date from the designated element and converts it using a helper function.
        It then checks whether the date falls within the allowed range defined by the crawler's start_date and end_date.
        If a post's date is out of range, the method logs the event and returns False immediately.
        Otherwise, it updates the current date if a new date is encountered, extracts the post's unique ID and URL,
        constructs a dictionary with these details, and appends it to the crawler's post_link list.
        
        Args:
            soup (BeautifulSoup): Parsed HTML content of the current page.
            cur_date (str): The current date string used for tracking and logging date transitions.
        
        Returns:
            Union[str, bool]: The last processed post date (formatted as a string) if posts are collected successfully,
            or False if a post's date is found to be out of the specified range.
        """
        posts = soup.select("tr.ub-content.us-post")
        
        for post in posts:
            # ë‚ ì§œ ê²€ì¦
            date = post.select_one("td.gall_date")['title'] if post.select_one("td.gall_date") else "ë‚ ì§œ ì—†ìŒ"
            date = date_type_for_search_result(date)
            
            if not is_date_in_range(date, self.start_date, self.end_date):
                logger.info(f"â—ï¸ Stopped by found date {str(date).split()[0]}")
                return False
            
            date = str(date).split()[0]
            
            # ë‚ ì§œ ë„˜ì–´ê°ˆ ì‹œ ë¡œê·¸ ì‘ì„±
            if date != cur_date:
                logger.info(f"ğŸ”— of {date}")
                cur_date = date
              
            gall_num = int(post.select_one("td.gall_num").get_text(strip=True))
            dc_url = "https://gall.dcinside.com"
            title_tag = post.select_one("td.gall_tit.ub-word a")
            link = dc_url + title_tag["href"] if title_tag else "ë§í¬ ì—†ìŒ"
            
            post_info = {
                "url" : link,
                "id" : gall_num,
                "date" : date
            }
            
            self.post_link.append(post_info)
        return date
    
    def page_traveler(self, driver:webdriver.Chrome, current_link:str):
        """
        Iterates through pagination pages to collect posts within the specified date range.
        
        Navigates the pages by loading the current link, parsing the HTML for post dates,
        and then locating the next page link. The process continues until the posts fall
        outside the desired date interval (from self.start_date to self.end_date). Random
        delays between page loads help mimic natural browsing.
        
        Parameters:
            driver (webdriver.Chrome): Selenium Chrome driver used for browser automation.
            current_link (str): URL of the current page where pagination begins.
        
        Returns:
            None.
        """
        # random_sleep_time = [0.8, 0.6, 0.7, 0.5]
        cur_date = self.end_date
        # i = 0
        
        while True:
            driver.get(current_link)
            time.sleep(WAIT_TIME - 1)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # is_crawl_post_success = False
            date = self.crawl_post_link(soup, cur_date)
            
            if date: # ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œë¥¼ ë§Œë‚  ë•Œ ê¹Œì§€ í¬ë¡¤ë§
                # í•œ í˜ì´ì§€ë¥¼ ë‹¤ ê¸ì—ˆìœ¼ë©´...
                current_page = soup.select_one('.bottom_paging_box.iconpaging em')
                dc_url = "https://gall.dcinside.com"
                next_link = current_page.find_next_sibling('a')
                current_link = dc_url + next_link['href']
                
                if str(next_link.get('class')) == "search_next": 
                    logger.info("Search next 10000 posts")
                
                # time.sleep(random_sleep_time[i := i % 4])
                time.sleep(random.randrange(500, 1000) / 1000)
                # i += 1
            
                cur_date = date    
                
            else: # íŠ¹ì • ë²”ìœ„ì˜ ë‚ ì§œë¥¼ ì „ë¶€ í¬ë¡¤ë§ í–ˆë‹¤ë©´
                logger.info(f"âœ… crawling {self.start_date} ~ {self.end_date} finished")
                break
        return
    
    def get_html_of_post(self, driver, url:str):
        """
        Retrieves HTML content of a post and parses it with BeautifulSoup.
        
        Loads the specified URL using a Selenium WebDriver instance and applies a randomized delay.
        Retries up to MAX_TRY times if the page source cannot be parsed, logging an error on each failure.
        Returns a BeautifulSoup object containing the parsed HTML if successful, or False if all attempts fail.
        
        Args:
            driver: A Selenium WebDriver instance used to load the webpage.
            url (str): The URL of the post to retrieve.
        
        Returns:
            BeautifulSoup: Parsed HTML content if the retrieval is successful.
            bool: False if all retry attempts fail.
        """
        headers = {'User-Agent': "Mozilla/5.0 (compatible; Daum/3.0; +http://cs.daum.net/)"}
        for _ in range(self.MAX_TRY):
            # response = requests.get(url, headers=headers)
            
            driver.get(url)
            time.sleep(WAIT_TIME - (random.randrange(50, 100)/100))
            soup = BeautifulSoup(driver.page_source, "html.parser")            
            if soup:
                return soup
            
            else:# í˜ì´ì§€ ì ‘ê·¼ ì¬ì‹œë„
                logger.error(f"âŒ {url} request FAILED!")
                time.sleep(self.RETRY_WAITS)
                continue
        return False
            
    def html_parser(self, driver:webdriver.Chrome, post_info:dict, parsed_post:BeautifulSoup):
        """
        Extracts post details, metadata, and comments from HTML content.
        
        Args:
            driver (webdriver.Chrome): Selenium Chrome driver used for dynamic webpage interactions.
            post_info (dict): Dictionary containing post metadata with keys such as 'url' and 'id'.
            parsed_post (BeautifulSoup): Parsed HTML object of the post page.
        
        Returns:
            dict: A dictionary with the extracted data including:
                - post_id (int): Unique identifier for the post.
                - post_url (str): URL of the post.
                - title (str): Title of the post.
                - content (str): Main text content of the post.
                - created_at (str): Post creation timestamp in ISO format (YYYY-MM-DDTHH:MM:SS).
                - view_count (int): Number of views.
                - upvote_count (int): Count of upvotes.
                - downvote_count (int): Count of downvotes.
                - comment_count (int): Total number of comments.
                - comments (list[dict]): List of comment dictionaries containing details such as comment_id, content, 
                  reply status, creation time, and vote counts.
        
        Example:
            >>> result = instance.html_parser(driver, {"url": "http://example.com/post/1", "id": 1}, soup)
            >>> print(result["title"])
        """
        print("Now Watching : " , driver.current_url)
        def parse_main_content(target_element):
            """
            Extracts post content along with upvote and downvote counts from an HTML element.
            
            Locates the container for the post content, retrieving the text with line breaks preserved.
            Also extracts the upvote count from the element with classes "up_num font_red" and
            the downvote count from the element with class "down_num", converting both to integers.
            
            Args:
                target_element (bs4.element.Tag): The BeautifulSoup element containing the post details.
            
            Returns:
                tuple: A tuple of three elements:
                    - content (str): The text content of the post with HTML line breaks converted to newlines.
                    - upvote (int): The number of upvotes.
                    - downvote (int): The number of downvotes.
            """
            write_div = target_element.find("div", class_="write_div")
            gaechu = int(target_element.find("p", class_="up_num font_red").get_text(strip=True))
            bichu = int(target_element.find("p", class_="down_num").get_text(strip=True))
            content = write_div.get_text(separator="\n", strip=True)  # <br>ì„ \nìœ¼ë¡œ ë³€í™˜, ê³µë°± ì œê±°
            return content, gaechu, bichu

        def parse_comments(soup:BeautifulSoup):
            """
            ëŒ“ê¸€ ë° ëŒ€ëŒ“ê¸€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.
            
            HTML íŒŒì‹± ê²°ê³¼ BeautifulSoup ê°ì²´ë¡œë¶€í„° ëŒ“ê¸€ ë° ëŒ€ëŒ“ê¸€ì„ ì°¾ì•„ ê° í•­ëª©ì„
            ë”•ì…”ë„ˆë¦¬ë¡œ ìˆ˜ì§‘í•œë‹¤. ìˆ˜ì§‘ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ëŠ” ë‹¤ìŒ í‚¤ë¥¼ í¬í•¨í•œë‹¤:
                - comment_id (int): ëŒ“ê¸€ ë˜ëŠ” ëŒ€ëŒ“ê¸€ì˜ ê³ ìœ  ì‹ë³„ì. ëŒ€ëŒ“ê¸€ì€ ìƒìœ„ ëŒ“ê¸€ì˜ ID ì‚¬ìš©.
                - content (str): ëŒ“ê¸€ ë˜ëŠ” ëŒ€ëŒ“ê¸€ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©.
                - is_reply (int): ëŒ“ê¸€ì€ 0, ëŒ€ëŒ“ê¸€ì€ 1.
                - created_at (str): ISO 8601 í˜•ì‹("yyyy-mm-ddThh:mm:ss")ìœ¼ë¡œ ë³€í™˜ëœ ì‘ì„± ì‹œê°„.
                - upvote_count (int): ì¶”ì²œ ìˆ˜, ê¸°ë³¸ê°’ì€ 0.
                - downvote_count (int): ë¹„ì¶”ì²œ ìˆ˜, ê¸°ë³¸ê°’ì€ 0.
            
            ê´‘ê³  ëŒ“ê¸€(í´ë˜ìŠ¤ì— "dory" í¬í•¨)ì€ ì œì™¸ëœë‹¤.
            
            Args:
                soup (BeautifulSoup): HTMLì´ íŒŒì‹±ëœ BeautifulSoup ê°ì²´.
            
            Returns:
                list[dict]: ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸.
            """
            comment_list = []
            comment_ul = soup.find("ul", class_="cmt_list")
            
            if not comment_ul:
                # print("no comments")
                return comment_list  # ëŒ“ê¸€ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            
            for li in comment_ul.find_all("li", recursive=False):  # ìµœìƒìœ„ lië§Œ íƒìƒ‰ (ëŒ€ëŒ“ê¸€ ì œì™¸)
                # ğŸ”¹ ëŒ“ê¸€ì¸ì§€ ëŒ€ëŒ“ê¸€ì¸ì§€ êµ¬ë¶„
                is_reply = 0  # ê¸°ë³¸ì ìœ¼ë¡œ ëŒ“ê¸€(0)
                
                if "dory" in li.get("class", []): # ê´‘ê³ ëŒ“ê¸€ ê±°ë¥´ê¸° (ëŒ“ê¸€ëŒì´ ê´‘ê³ )
                    continue
                
                # ğŸ”¹ ëŒ“ê¸€ ë‚´ìš©
                if (cmt_id := li.get('id')) and not li.select_one("p.del_reply"): # ëŒ“ê¸€ì´ë©´
                    content_tag = li.select_one("p.usertxt.ub-word")
                    content = content_tag.get_text(strip=True) if content_tag else ""

                    # ğŸ”¹ ì‘ì„± ì‹œê°„ (datetime ë³€í™˜)
                    created_at = li.select_one("span.date_time").get_text(strip=True)
                    # isoformatìœ¼ë¡œ ë³€í™˜
                    ymd, hms = md_to_ymd(created_at).split()
                    created_at = 'T'.join([convert_date_format(ymd), hms])
                    
                    comment_id = int(cmt_id.split('_')[-1])
                    
                    # ğŸ”¹ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    comment_list.append({
                        "comment_id": comment_id,
                        "content": content,
                        "is_reply": is_reply,
                        "created_at": created_at,
                        "upvote_count": 0,
                        "downvote_count": 0
                    })
                else:
                    comment_id = None
                
                if li.find("div", class_="reply_box"):
                    is_reply = 1  # ëŒ€ëŒ“ê¸€(1)
                # ğŸ”¹ ëŒ€ëŒ“ê¸€ íƒìƒ‰
                reply_ul = li.select_one("ul.reply_list")
                
                if reply_ul:
                    reply_parent_id = int(reply_ul.get('id').split('_')[-1])
                    for reply_li in reply_ul.find_all("li", class_="ub-content"):
                        # reply_parent_id = comment_id
                        if reply_content_tag := reply_li.select_one("p.usertxt.ub-word"):
                            reply_content = reply_content_tag.get_text(strip=True) if reply_content_tag else ""

                            reply_created_at = reply_li.select_one("span.date_time").get_text(strip=True)
                            ymd, hms = md_to_ymd(reply_created_at).split()
                            reply_created_at = 'T'.join([convert_date_format(ymd), hms])

                            comment_list.append({
                                "comment_id": reply_parent_id,
                                "content": reply_content,
                                "is_reply": 1,  # ëŒ€ëŒ“ê¸€
                                "created_at": reply_created_at,
                                "upvote_count": 0,
                                "downvote_count": 0                            
                            })
                        else: continue

            return comment_list

        def scrape_all_comment_pages(driver, soup):
            """
            Aggregates all comments across paginated comment pages.
            
            Extracts the total number of comments from the initial BeautifulSoup object and collects comments from the
            first page using a helper function. If pagination is found, iterates over each numeric page button by executing
            its JavaScript to load subsequent comment pages, waits for the new content to load, and accumulates the comments.
            
            Args:
                driver (webdriver.Chrome): Selenium WebDriver used to execute JavaScript and navigate between pages.
                soup (BeautifulSoup): Parsed HTML content of the initial comment page.
            
            Returns:
                tuple:
                    int: The total comment count as indicated on the page.
                    list: A list of comments collected from all paginated comment pages.
            """
            comment_count_tag = soup.find('span', class_='gall_comment')
            comment_count = int(comment_count_tag.find('a').text[len("ëŒ“ê¸€ "):]) if comment_count_tag else 0
            
            all_comments = []  # ëª¨ë“  ëŒ“ê¸€ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

            # ğŸ”¹ ì²« ë²ˆì§¸ í˜ì´ì§€ ëŒ“ê¸€ ìˆ˜ì§‘
            comments = parse_comments(soup)
            all_comments.extend(comments)
            

            # ğŸ”¹ ë‹¤ìŒ ëŒ“ê¸€ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°
            paging_box = soup.select_one("div.cmt_paging")
            if not paging_box:
                # print("ëŒ“ê¸€ í˜ì´ì§€ë„¤ì´ì…˜ì´ ì—†ìŒ.")
                return comment_count, all_comments

            next_page_btns = paging_box.find_all("a", href=True)

            for btn in next_page_btns:
                page_number = btn.get_text(strip=True)
                if page_number.isdigit():
                    # print(f"ì´ë™ ì¤‘: ëŒ“ê¸€ í˜ì´ì§€ {page_number}")

                    # ğŸ”¹ JavaScript ì‹¤í–‰í•˜ì—¬ ëŒ“ê¸€ í˜ì´ì§€ ì´ë™
                    driver.execute_script(btn["href"])

                    # ğŸ”¹ ìƒˆë¡œìš´ í˜ì´ì§€ HTMLì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ëŒ€ê¸°
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.cmt_paging"))
                    )

                    # ğŸ”¹ ìƒˆë¡œìš´ soup ì—…ë°ì´íŠ¸ í›„ ëŒ“ê¸€ ì¶”ê°€ ìˆ˜ì§‘
                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    comments = parse_comments(soup)
                    all_comments.extend(comments)

            return comment_count, all_comments

        post_url = post_info['url']
        post_id = post_info['id']
        # type : yyyy.mm.dd HH:MM:SS
        created_at = parsed_post.find("span", class_="gall_date").get_text(strip=True)
        ymd, hms = created_at.split()
        created_at = 'T'.join([convert_date_format(ymd), hms])
        # created_at.replace('-', '.') --> ì•„ë‹ˆ ì´ê²Œ ì‚´ì•„ìˆì—ˆëŠ”ë°ã…” ì–´ë–»ê²Œ 23-08-07 ì´ëŸ° ì‹ìœ¼ë¡œ ì €ì¥ëœê±°ì§€?
        # created_at = datetime.strptime(created_at, "%Y.%m.%d %H:%M:%S")
        title = parsed_post.find("span", class_="title_subject").get_text(strip=True)
        view_count = int(parsed_post.find("span", class_="gall_count").get_text(strip=True)[len("ì¡°íšŒ "):])
        content, up_vote, down_vote = parse_main_content(parsed_post)
        comment_count, comment_list = scrape_all_comment_pages(driver, parsed_post)
        
        parsed_finally = {
            "post_id" : post_id ,
            "post_url" : post_url,
            "title" : title,
            "content" : content,
            "created_at" : created_at,
            "view_count" : view_count,
            "upvote_count" : up_vote,
            "downvote_count" : down_vote,
            "comment_count" : comment_count,
            "comments" : comment_list
        }
                
        return parsed_finally
    
    def save_json(self, parsed_json:json, post_info:dict):
        """
        Saves parsed JSON data to a file in a structured directory based on car ID and post date.
        
        Args:
            parsed_json (json): The JSON data to be saved.
            post_info (dict): Metadata dictionary that must include:
                - 'date': A date string where the substring from index 5 is used to form the directory.
                - 'id': A unique identifier used as the JSON file name.
        
        The file path is formatted as:
          "extracted/{car_id}/{file_date}/raw/dcinside/{id}.json",
        where {file_date} is derived from the 'date' string in post_info.
        
        Directories are created if they do not exist. Any exceptions during the file write
        are caught and logged; no exception is propagated.
        """
        file_date = post_info['date'][5:]
        file_path = f"extracted/{self.car_id}/{file_date}/raw/dcinside/{post_info['id']}.json"
        directory = os.path.dirname(file_path)

        
        if not os.path.exists(directory):  # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´
            os.makedirs(directory)  # ë””ë ‰í† ë¦¬ ìƒì„±
        
        try:
            with open(file_path, "w", encoding="utf-8") as file:
                # file.write(html_source)
                json.dump(parsed_json, file, ensure_ascii=False, indent=4)
            # print(f"HTML ì†ŒìŠ¤ê°€ {file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")   
        
    def run_crawl(self,):
        # ë“œë¼ì´ë²„ ì„¸íŒ…
        """
        Execute the complete crawl sequence.
        
        Initializes the Selenium driver and navigates to the search entry URL within the specified date range.
        Collects valid post links by traversing the pagination, retrieves each post's HTML content, parses the
        required details, and saves the extracted data in JSON format. A randomized delay between posts is used
        to emulate natural browsing behavior.
        """
        driver=self._get_driver()
        logger.info("âœ… Driver Successfully Set.")
        
        # ê²€ìƒ‰ ê¸°ê°„ ë‚´ ê°€ì¥ ìµœì‹  ê²Œì‹œê¸€ ê²€ìƒ‰ ê²°ê³¼ ì ‘ê·¼
        end_point = self.get_entry_point(driver, url=self.search_url)
        logger.info("âœ… Successfully accessed to init date")
        
        # ì ‘ê·¼ ìœ„ì¹˜ë¡œë¶€í„° ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘
        self.page_traveler(driver, end_point)
        
        # ìˆ˜ì§‘ëœ ë§í¬ë¥¼ ë°©ë¬¸í•˜ë©° html ì†ŒìŠ¤ ì €ì¥
        for i, post in enumerate(self.post_link):
            # print(post['url'])
            # print(f"Progressing... [{i+1} / {len(self.post_link)}]")
            
            # random_sleep_time = [0.8, 0.6, 0.7, 0.5]
            parsed_source = self.get_html_of_post(driver, post['url'])
            res_json = self.html_parser(driver, post, parsed_source)
            
            logger.info(f"ğŸ’¿ â Saving...[{i+1} / {len(self.post_link)}]")
            self.save_json(res_json, post)
                
            time.sleep(1 + random.randrange(50, 100) / 100)
                    

    
if __name__=="__main__":
    
    car = {'ì‚°íƒ€í˜': [ # ì°¨ì¢…
                'ì‚°íƒ€í˜', # í•´ë‹¹ ì°¨ì¢…ì˜ ì´ëª…
                'ì‹¼íƒ€í˜']
        }   
  
    s_date="2024-12-30"
    e_date="2025-01-02"
    
    daily_batch = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    logger.info(f"âœ… Initiating Crawler : {s_date} ~ {e_date}")
    
    # car_keywordëŠ” lambda_handlerì—ì„œ eventë¡œ ì²˜ë¦¬í•˜ê²Œ í•  ê²ƒ
    crawler = DC_crawler(s_date, e_date, car_id="casper", car_keyword="ìºìŠ¤í¼", is_daily_batch=True)
    
    logger.info("Running crawler")
    crawler.run_crawl()
    
    logger.info("âœ… Crawling Finished")
    