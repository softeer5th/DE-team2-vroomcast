from selenium import webdriver
from selenium.webdriver import Chrome
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from datetime import datetime
import time, json, logging, os
from bs4 import BeautifulSoup
import boto3, random

logging.basicConfig(level=logging.INFO)  # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
logger = logging.getLogger(__name__)


BASE_URL = "https://gall.dcinside.com/board/lists/?id=car_new1"
WAIT_TIME = 2

# ì œëª©ë§Œ / ì œëª©+ë‚´ìš©
SEARCH_URL_TITLE = f"https://gall.dcinside.com/board/lists/?id=car_new1&s_type=search_subject&s_keyword="
SEARCH_URL_TITLE_AND_CONTENT = f"https://gall.dcinside.com/board/lists/?id=car_new1&s_type=search_subject_memo&s_keyword="  
    
def convert_date_format(date_str:str):
    """
    yyyy-mm-dd HH:MM:SS -> yyyy-mm-ddTHH:MM:SS (ISO Format)
    """
    return 'T'.join(date_str.split())

def md_to_ymd(date_str:str):
    """
    ëŒ“ê¸€ íƒ€ì„ìŠ¤íƒ¬í”„ì˜ ë‘ ê°€ì§€ ë‚ ì§œ í˜•ì‹ì„ ì…ë ¥ë°›ì•„ "yyyy-mm-dd HH:MM:SS" í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë³¸ë¬¸ ë° ëŒ“ê¸€ì˜ ë‚ ì§œ í˜•ì‹ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
    
    Args:
        date_str: ë³€í™˜í•  ë‚ ì§œ ë¬¸ìì—´ ("yyyy.mm.dd HH:MM:SS" ë˜ëŠ” "mm.dd HH:MM:SS" í˜•ì‹)

    Returns:
        "yyyy-mm-dd HH:MM:SS" í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ë‚ ì§œ ë¬¸ìì—´
    """
    try:
        # "yyyy.mm.dd HH:MM:SS" í˜•ì‹ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        date_obj = datetime.strptime(date_str, "%Y.%m.%d %H:%M:%S")
        return date_obj.strftime("%Y-%m-%d %H:%M:%S")
    
    except ValueError:
        try:
            # "mm.dd HH:MM:SS" í˜•ì‹ì¸ ê²½ìš° "2025.mm.dd HH:MM:SS"ë¡œ ë³€í™˜ í›„ ì ìš©
            date_obj = datetime.strptime(date_str, "%m.%d %H:%M:%S")
            return date_obj.replace(year=datetime.now().year).strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            return "Invalid date format"

def is_time_in_range(time_str, start_time, end_time):
    """
    ì…ë ¥ëœ ì‹œê°„ ë¬¸ìì—´ì´ start_timeê³¼ end_time ì‚¬ì´ì— ìˆëŠ”ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜.

    Args:
        time_str: "%Y-%m-%d %H:%M:%S" í˜•ì‹ì˜ ì‹œê°„ ë¬¸ìì—´.

    Returns:
        True: ì…ë ¥ëœ ì‹œê°„ì´ start_timeê³¼ end_time ì‚¬ì´ì— ìˆëŠ” ê²½ìš°.
        False: ì…ë ¥ëœ ì‹œê°„ì´ start_timeê³¼ end_time ì‚¬ì´ì— ì—†ëŠ” ê²½ìš°.
    """

    try:
        input_time = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        return False  # ì˜ëª»ëœ í˜•ì‹ì˜ ë¬¸ìì—´

    start_time = datetime.strptime(start_time, "%Y-%m-%d %H:%M:%S")
    end_time = datetime.strptime(end_time, "%Y-%m-%d %H:%M:%S")

    if start_time <= input_time <= end_time:
        return "IN"
    elif input_time > end_time: 
        return "OVER"
    else: return "UNDER"

    # return start_time <= input_time <= end_time  

def get_driver():
    chrome_options = ChromeOptions()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-dev-tools")
    chrome_options.add_argument("--no-zygote")
    chrome_options.add_argument("--single-process")
    chrome_options.add_argument("--verbose")
    chrome_options.binary_location = "/opt/chrome/chrome-linux64/chrome"
    # prefs = {
    #     "profile.managed_default_content_settings.images": 2,  # ì´ë¯¸ì§€ ë¹„í™œì„±í™”
    #     "profile.managed_default_content_settings.ads": 2,     # ê´‘ê³  ë¹„í™œì„±í™”
    #     "profile.managed_default_content_settings.media": 2    # ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ë¹„í™œì„±í™”
    # }
    # chrome_options.add_experimental_option("prefs", prefs)

    service = Service(
        executable_path="/opt/chrome-driver/chromedriver-linux64/chromedriver",
        # service_log_path="/tmp/chromedriver.log"
    )
    driver = Chrome(
        service=service,
        options=chrome_options
    )

    return driver  
    
class DC_crawler:
    MAX_TRY = 2
    RETRY_WAITS = 2
    post_link = [
    ]
    
    def __init__(self, s_date, e_date, car_id, car_keyword, bucket_name, batch, folder_date):
        self.start_date = s_date
        self.end_date = e_date
        self.car_id = car_id
        self.keyword = car_keyword
        self.search_url = [SEARCH_URL_TITLE + kw for kw in car_keyword]
        self.BUCKET_NAME = bucket_name
        self.folder_date = folder_date
        self.batch = batch
        self.id_check = []
        self.s3 = boto3.client("s3")
        
    
    def get_entry_point(self, driver:webdriver.Chrome, url):
        """
        ë””ì‹œì¸ì‚¬ì´ë“œì—ì„œ 'ë¹ ë¥¸ ì´ë™'ê¸°ëŠ¥ì„ í†µí•´ ê²€ìƒ‰ ê²°ê³¼ë“¤ ì¤‘ ê²€ìƒ‰ ê¸°ê°„ì˜ ë§ˆì§€ë§‰ ë‚ ì§œë¡œ ì´ë™í•˜ëŠ” urlì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        Returns:
            ì´ë™í•  í˜ì´ì§€ url
        """
        e_date = self.end_date.split()[0]
        
        driver.get(url)
        time.sleep(WAIT_TIME)
        #-----------------------------------------------
        # ğŸ”¹ 1. ë‚ ì§œ ê²€ìƒ‰ ì°½ì„ ì—¬ëŠ” ë²„íŠ¼ í´ë¦­
        #-----------------------------------------------
        open_date_picker = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "button.btn_grey_roundbg.btn_schmove")))
        open_date_picker.click()
        time.sleep(1)  # ê²€ìƒ‰ ì°½ì´ ëœ¨ëŠ” ì‹œê°„ ê³ ë ¤
        
        #-----------------------------------------------
        # ğŸ”¹ 2. ë‚ ì§œ ì…ë ¥ í•„ë“œ ì°¾ê¸°
        #-----------------------------------------------
        date_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "input.dayin.calendar")))
        
        #-----------------------------------------------
        # ğŸ”¹ 3. ë‚ ì§œ ì…ë ¥
        #-----------------------------------------------
        target_date = e_date  # ê²€ìƒ‰í•  ë‚ ì§œ
        # JavaScriptë¡œ ë‚ ì§œ ê°’ ë³€ê²½
        driver.execute_script("arguments[0].value = arguments[1];", date_input, target_date)
        date_input.send_keys(target_date)
        date_input.send_keys(Keys.RETURN)  # ì—”í„° ì…ë ¥

        #-----------------------------------------------
        # ğŸ”¹ 4. ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ í›„ ë¡œë”© ëŒ€ê¸°
        #-----------------------------------------------
        search_btn = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "button.btn_blue.small.fast_move_btn"))
        )
        search_btn.click()
        time.sleep(0.5)  # ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì— ë”°ë¼ ì¡°ì •
        
        #-----------------------------------------------
        # ğŸ”¹ 5. í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        #-----------------------------------------------
        current_page_url = driver.current_url
        return current_page_url        
        
    def crawl_post_link(self, soup:BeautifulSoup, cur_date:str):
        """
        í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ë“¤ì˜ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ê²€ìƒ‰ ë‚ ì§œ ë²”ìœ„ë¥¼ ë„˜ì–´ê°€ë©´ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
        ë§í¬ëŠ” {
            'url': ê²Œì‹œê¸€ url,
            'id': ê²Œì‹œê¸€ id,
            'date': y-m-d H:M:S
        }ì˜ í˜•íƒœë¡œ ìˆ˜ì§‘ë˜ì–´ post_linkì— ì ì¬ë©ë‹ˆë‹¤.
        """
        posts = soup.select("tr.ub-content.us-post")
        
        for post in posts:
            # ë‚ ì§œ ê²€ì¦
            date = post.select_one("td.gall_date")['title'] if post.select_one("td.gall_date") else "0000-00-00 00:00:00"
            
            time_checker = is_time_in_range(date, self.start_date, self.end_date)
            if time_checker == "UNDER":
                logger.info(f"â—ï¸ ë‚ ì§œ {str(date)} ë¥¼ ë°œê²¬í•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return False
            elif time_checker == "OVER":
                logger.info(f"â—ï¸ ì´ ê²Œì‹œë¬¼({str(date)})ì€ ê²€ìƒ‰ ë‚ ì§œë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                continue
            
            ymd_date = str(date).split()[0]
            
            # ë‚ ì§œ ë„˜ì–´ê°ˆ ì‹œ ë¡œê·¸ ì‘ì„±
            if ymd_date != cur_date:
                logger.info(f"{ymd_date}ì˜ ë§í¬ë¥¼ ìˆ˜ì§‘ ì¤‘ ğŸ”—")
                print(f"{ymd_date}ì˜ ë§í¬ë¥¼ ìˆ˜ì§‘ ì¤‘ ğŸ”—")
                cur_date = date
              
            gall_num = int(post.select_one("td.gall_num").get_text(strip=True))
            dc_url = "https://gall.dcinside.com"
            title_tag = post.select_one("td.gall_tit.ub-word a")
            link = dc_url + title_tag["href"] if title_tag else "ë§í¬ ì—†ìŒ"
            
            if gall_num not in self.id_check:
                self.id_check.append(link)
                post_info = {
                    "url" : link,
                    "id" : gall_num,
                    "date" : date # y-m-d H:M:S
                }
            
                self.post_link.append(post_info)
            else:
                logger.info("ë§í¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                continue
        return ymd_date
    
    def page_traveler(self, driver:webdriver.Chrome, current_link:str):
        """
        í˜ì´ì§• ë°•ìŠ¤ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤. <br>
        ì‹œê°„ **ì—­ìˆœ**ìœ¼ë¡œ ìˆœíšŒí•©ë‹ˆë‹¤. <br>
        (í˜ì´ì§• ë°•ìŠ¤ëŠ” ì •ë°©í–¥(1â†’2â†’3...) ìˆœíšŒ, ë³´ì´ëŠ” ê²Œì‹œê¸€ì€ ì‹œê°„ ì—­ìˆœ)
        """
        cur_date = self.end_date
        
        while True:
            driver.get(current_link)
            time.sleep(WAIT_TIME - 1)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            date = self.crawl_post_link(soup, cur_date)
            
            if date: # ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œë¥¼ ë§Œë‚  ë•Œ ê¹Œì§€ í¬ë¡¤ë§
                # í•œ í˜ì´ì§€ë¥¼ ë‹¤ ê¸ì—ˆìœ¼ë©´...
                current_page = soup.select_one('.bottom_paging_box.iconpaging em')
                dc_url = "https://gall.dcinside.com"
                next_link = current_page.find_next_sibling('a')
                # ë‹¤ìŒ í˜ì´ì§• ë°•ìŠ¤ê°€ ê°€ë¦¬í‚¤ëŠ” ë§í¬ë¡œ ì´ë™
                current_link = dc_url + next_link['href']

                time.sleep(random.randrange(50, 100) / 100)            
                cur_date = date    
                
            else: # íŠ¹ì • ë²”ìœ„ì˜ ë‚ ì§œë¥¼ ì „ë¶€ í¬ë¡¤ë§ í–ˆë‹¤ë©´
                logger.info(f"âœ… Crawling {self.start_date} ~ {self.end_date} finished")
                print(f"âœ… Crawling {self.start_date} ~ {self.end_date} finished")
                break
        return
    
    def get_html_of_post(self, driver:webdriver.Chrome, url:str):
        """
        ê° ê²Œì‹œê¸€ì˜ html sourceë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        ê°€ì ¸ì˜¨ sourceë¥¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        for i in range(self.MAX_TRY):
            try:
                driver.get(url)
                time.sleep(WAIT_TIME - (random.randrange(50, 100) / 100))
                soup = BeautifulSoup(driver.page_source, "html.parser")
                if soup:
                    return soup
            
            except:# í˜ì´ì§€ ì ‘ê·¼ ì¬ì‹œë„
                logger.error(f"âŒ {url} request ì‹¤íŒ¨! ë‚¨ì€ ì¬ì‹œë„ íšŸìˆ˜: {self.MAX_TRY -1 -i}")
                print(f"âŒ {url} request ì‹¤íŒ¨! ë‚¨ì€ ì¬ì‹œë„ íšŸìˆ˜: {self.MAX_TRY -1 -i}")
                time.sleep(self.RETRY_WAITS)
                continue
        return False
            
    def html_parser(self, driver:webdriver.Chrome, post_info:dict, parsed_post:BeautifulSoup):
        """
        ê²Œì‹œê¸€ì˜ id, url, ì œëª©, ë‚´ìš©, ì‘ì„± ì‹œê°„, ì¡°íšŒìˆ˜, ì¶”ì²œ, ë¹„ì¶”ì²œ, ëŒ“ê¸€ ìˆ˜ ë° ëŒ“ê¸€ì„ 
        ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        print("Now Parsing â–¶ " , driver.current_url)

        def parse_main_content(target_element:BeautifulSoup):
            """
            ê²Œì‹œê¸€ ë³¸ë¬¸ í¬ë¡¤ë§
            Returns:
                ë³¸ë¬¸ ë‚´ìš©, ì¶”ì²œ ìˆ˜, ë¹„ì¶” ìˆ˜
            """
            write_div = target_element.find("div", class_="write_div")
            thumb = int(target_element.find("p", class_="up_num font_red").get_text(strip=True))
            r_thumb = int(target_element.find("p", class_="down_num").get_text(strip=True))
            content = write_div.get_text(separator="\n", strip=True)  # <br>ì„ \nìœ¼ë¡œ ë³€í™˜, ê³µë°± ì œê±°
            return content, thumb, r_thumb

        def parse_comments(soup:BeautifulSoup):
            """
            ëŒ“ê¸€ ë° ëŒ€ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
            
            Args:
                soup (BeautifulSoup): BeautifulSoupìœ¼ë¡œ íŒŒì‹±ëœ ê²Œì‹œê¸€
            
            Returns:
                list[dict]: ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸
            """
            comment_list = []
            comment_ul = soup.find("ul", class_="cmt_list")
            
            if not comment_ul:
                return comment_list  # ëŒ“ê¸€ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

            for li in comment_ul.find_all("li", recursive=False):  # ìµœìƒìœ„ lië§Œ íƒìƒ‰ (ëŒ€ëŒ“ê¸€ ì œì™¸)
                # ğŸ”¹ ëŒ“ê¸€ì¸ì§€ ëŒ€ëŒ“ê¸€ì¸ì§€ êµ¬ë¶„
                is_reply = 0  # ê¸°ë³¸ì ìœ¼ë¡œ ëŒ“ê¸€(0)
                
                if "dory" in li.get("class", []): # ê´‘ê³ ëŒ“ê¸€ ê±°ë¥´ê¸° (ëŒ“ê¸€ëŒì´ ê´‘ê³ )
                    continue                      

                # ğŸ”¹ ëŒ“ê¸€ ë‚´ìš©
                if (cmt_id := li.get('id')) and not li.select_one("p.del_reply"): # ëŒ“ê¸€ì´ë©´
                    content_tag = li.select_one("p.usertxt.ub-word")
                    content = content_tag.get_text(strip=True) if content_tag else ""

                    # ğŸ”¹ ì‘ì„± ì‹œê°„
                    created_at = li.select_one("span.date_time").get_text(strip=True) 
                    # isoformatìœ¼ë¡œ ë³€í™˜
                    created_at = convert_date_format(md_to_ymd(created_at))
                    
                    comment_id = int(cmt_id.split('_')[-1])
                    
                    # ğŸ”¹ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    comment_list.append({
                        "comment_id": comment_id,
                        "content": content,
                        "is_reply": is_reply,
                        "created_at": created_at,
                        # ë””ì‹œì¸ì‚¬ì´ë“œëŠ” ëŒ“ê¸€ì— ì¶”ì²œ/ë¹„ì¶”ì²œì´ ì—†ìŒ
                        "upvote_count": 0, 
                        "downvote_count": 0
                    })
                else:
                    comment_id = None

                if li.find("div", class_="reply_box"):
                    is_reply = 1  # ëŒ€ëŒ“ê¸€(1)
                    
                # ğŸ”¹ ëŒ€ëŒ“ê¸€ íƒìƒ‰
                reply_ul = li.select_one("ul.reply_list")
                
                if reply_ul:
                    reply_parent_id = int(reply_ul.get('id').split('_')[-1])
                    for reply_li in reply_ul.find_all("li", class_="ub-content"):
                        if reply_content_tag := reply_li.select_one("p.usertxt.ub-word"):
                            reply_content = reply_content_tag.get_text(strip=True) if reply_content_tag else ""
                            reply_created_at = reply_li.select_one("span.date_time").get_text(strip=True)                    
                            reply_created_at = convert_date_format(md_to_ymd(reply_created_at))
                            comment_list.append({
                                "comment_id": reply_parent_id,
                                "content": reply_content,
                                "is_reply": 1,  # ëŒ€ëŒ“ê¸€
                                "created_at": reply_created_at,
                                "upvote_count": 0,
                                "downvote_count": 0                            
                            })
                        else: continue

            return comment_list

        def scrape_all_comment_pages(driver:webdriver.Chrome, soup:BeautifulSoup):
            """
            ì£¼ì–´ì§„ soupì„ ê¸°ë°˜ìœ¼ë¡œ ëŒ“ê¸€ í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ëª¨ë“  ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜.
            Returns:
                ëŒ“ê¸€ ê°œìˆ˜, ìˆ˜ì§‘ëœ ëŒ“ê¸€ ë° ëŒ€ëŒ“ê¸€ ëª¨ìŒ
            """
            comment_count_tag = soup.find('span', class_='gall_comment')
            comment_count = int(comment_count_tag.find('a').text[len("ëŒ“ê¸€ "):]) if comment_count_tag else 0
            
            all_comments = []  # ëª¨ë“  ëŒ“ê¸€ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

            # ğŸ”¹ ì²« ë²ˆì§¸ í˜ì´ì§€ ëŒ“ê¸€ ìˆ˜ì§‘
            comments = parse_comments(soup)
            all_comments.extend(comments)
            

            # ğŸ”¹ ëŒ“ê¸€ í˜ì´ì§• ë°•ìŠ¤ ì°¾ê¸°
            paging_box = soup.select_one("div.cmt_paging")

            next_page_btns = paging_box.find_all("a", href=True)
            # í˜„ ëŒ“ê¸€ í˜ì´ì§€ëŠ” <em> tagì´ë¯€ë¡œ ëŒ“ê¸€ í˜ì´ì§€ê°€ 2 ì´ìƒì¼ ë•Œë§Œ ì´í•˜ì˜ forë¬¸ ì‘ë™

            for btn in next_page_btns:
                page_number = btn.get_text(strip=True)
                if page_number.isdigit():

                    # ğŸ”¹ JavaScript ì‹¤í–‰í•˜ì—¬ ëŒ“ê¸€ í˜ì´ì§€ ì´ë™
                    # ê²Œì‹œê¸€ í˜ì´ì§€ë„¤ì´ì…˜ê³¼ ë‹¬ë¦¬ javascriptë¡œ ë˜ì–´ ìˆìŒ
                    # <a href="javascript:viewComments(2,'D')">2</a> 
                    driver.execute_script(btn["href"])

                    # ğŸ”¹ ìƒˆë¡œìš´ ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ëŒ€ê¸°
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.cmt_paging"))
                    )

                    # ğŸ”¹ ìƒˆë¡œìš´ soup ì—…ë°ì´íŠ¸ í›„ ëŒ“ê¸€ ì¶”ê°€ ìˆ˜ì§‘
                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    comments = parse_comments(soup)
                    all_comments.extend(comments)

            return comment_count, all_comments

        post_url = post_info['url']
        post_id = post_info['id']
        created_at = convert_date_format(post_info['date'])
        
        title = parsed_post.find("span", class_="title_subject").get_text(strip=True)
        view_count = int(parsed_post.find("span", class_="gall_count").get_text(strip=True)[len("ì¡°íšŒ "):])
        content, up_vote, down_vote = parse_main_content(parsed_post)
        comment_count, comment_list = scrape_all_comment_pages(driver, parsed_post)
        
        parsed_finally = {
            "post_id" : post_id ,
            "post_url" : post_url,
            "title" : title,
            "content" : content,
            "created_at" : created_at,
            "view_count" : view_count,
            "upvote_count" : up_vote,
            "downvote_count" : down_vote,
            "comment_count" : comment_count,
            "comments" : comment_list
        }
                
        return parsed_finally
  

    def save_json(self, parsed_json:json, post_id:int):
        """
        ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì— ëŒ€í•œ ì •ë³´ë¥¼ S3ì— .jsonìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        file_path = f"extracted/{self.car_id}/{self.folder_date}/{self.batch}/raw/dcinside/{post_id}.json"
        directory = os.path.dirname(file_path)

        # 1. í´ë” ì¡´ì¬ í™•ì¸
        try:
            self.s3.head_object(Bucket=self.BUCKET_NAME, Key=directory)
            print("âœ… S3 í´ë” ê²½ë¡œ í™•ì¸ë¨")
        except:  # í´ë”ê°€ ì—†ëŠ” ê²½ìš°
            print(f"âŒ í´ë” ê²½ë¡œ ì—†ìŒ. ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘...{directory}")
            
        try:
            web_data = json.dumps(parsed_json, ensure_ascii=False, indent=4)
            print(f"âœ… Post ID: {post_id} â†’ íŒŒì¼ ìƒì„±ë¨")
            
        except Exception as e:
            print(f"âŒ json.dumps() ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")       
            
        try: # S3ì— JSONìœ¼ë¡œ ì €ì¥
            self.s3.put_object(
                Bucket = self.BUCKET_NAME,
                Key = file_path,
                Body = web_data,
                ContentType = "application/json"
            )     
            logger.info(f"âœ… Successfully uploaded {post_id}.json to s3-bucket")
            print(f"âœ… Successfully uploaded {post_id}.json to s3-bucket")

        except Exception as e:
            logger.error(f"âŒ S3ì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            print(f"âŒ S3ì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        
    def run_crawl(self,):
        """
        í¬ë¡¤ë§ ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
        """
        # ë“œë¼ì´ë²„ ì„¸íŒ…
        try:
            driver = get_driver()
        except:
            print("ğŸŸ¥ Check Driver Settings! ğŸŸ¥")
            exit(0)

        for url in self.search_url:
            # ê²€ìƒ‰ ê¸°ê°„ ë‚´ ê°€ì¥ ìµœì‹  ê²Œì‹œê¸€ ê²€ìƒ‰ ê²°ê³¼ ì ‘ê·¼
            end_point = self.get_entry_point(driver, url=url)
            if end_point:
                logger.info(f"âœ… {self.end_date}ì— ì ‘ê·¼ ì„±ê³µ")
                print(f"âœ… {self.end_date}ì— ì ‘ê·¼ ì„±ê³µ")
            else:
                logger.warning((f"âŒ {self.end_date}ì— ì ‘ê·¼ ì‹¤íŒ¨"))
                print(f"âŒ {self.end_date}ì— ì ‘ê·¼ ì‹¤íŒ¨")
                
            # ì ‘ê·¼ ìœ„ì¹˜ë¡œë¶€í„° ê³¼ê±°ë¡œ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘
            self.page_traveler(driver, end_point)
            print(f"âœ… ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ : {len(self.post_link)} links")
        
        # ìˆ˜ì§‘ëœ ë§í¬ë¥¼ ë°©ë¬¸í•˜ë©° html ì†ŒìŠ¤ ì €ì¥
        failed = 0
        for i, post in enumerate(self.post_link):

            try:
                parsed_source = self.get_html_of_post(driver, post['url'])
                res_json = self.html_parser(driver, post, parsed_source)
                
                logger.info(f"Saving...[{i+1} / {len(self.post_link)}]")
                print(f"Saving...[{i+1} / {len(self.post_link)}]")
                self.save_json(res_json, post['id'])
            except:
                failed += 1
                continue
                
            time.sleep(random.randrange(0, 50) / 100)
        
        driver.close()
        return failed, len(self.post_link)  

def lambda_handler(event, context):
    init_time = time.time()
    
    BUCKET_NAME = event.get('bucket') # my_s3_bucket_id
    car_id      = event.get('car_id') 
    car_keyword = event.get('keywords') # list[str]
    date        = event.get('date')
    batch       = event.get('batch')
    s_date      = event.get('start_datetime')
    e_date      = event.get('end_datetime')
        
    s_date = ' '.join(s_date.split('T'))
    e_date = ' '.join(e_date.split('T'))
    
    logger.info(f"âœ… í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±. {s_date} ~ {e_date} ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print(f"âœ… í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±. {s_date} ~ {e_date} ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    crawler = DC_crawler(s_date, e_date, car_id=car_id, car_keyword=car_keyword, bucket_name=BUCKET_NAME, batch=batch, folder_date=date)
    
    print("â–¶â–¶ í¬ë¡¤ë§ ì‹œì‘...")
    logger.info("â–¶â–¶ í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        failed, tried = crawler.run_crawl()
        logger.info("âœ… í¬ë¡¤ë§ ì¢…ë£Œ")
        print("âœ… í¬ë¡¤ë§ ì¢…ë£Œ")
        finished_time = time.time()
        delta = finished_time - init_time
        
        return {
            "statusCode": 200,
            "body": {
                "success": True,
                "end_time": convert_date_format(datetime.now().strftime("%y-%m-%d %H:%M:%S")),
                "duration": delta,
                "car_id": car_id,
                "date": date,
                "batch": batch,
                "start_datetime": s_date,
                "end_datetime": e_date,
                "attempted_posts_count": tried,
                "extracted_posts_count": tried - failed                
                }
        }        
    except Exception as e:
        logger.info("âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        print("âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        finished_time = time.time()
        delta = finished_time - init_time
        return {
            "statusCode": 500,
            "body": {
                "success": False,
                "end_time": convert_date_format(datetime.now().strftime("%y-%m-%d %H:%M:%S")),
                "duration": delta,
                "car_id": car_id,
                "date": date,
                "batch": batch,
                "start_datetime": s_date,
                "end_datetime": e_date,
                "attempted_posts_count": tried,
                "extracted_posts_count": tried - failed,                
                "Error": e
                }
        }  
