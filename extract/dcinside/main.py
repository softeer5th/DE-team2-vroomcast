from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
# from webdriver_manager.chrome import ChromeDriverManager # chromeë¸Œë¼ìš°ì € ë²„ì „ì— ë§ëŠ” ë“œë¼ì´ë²„ì¸ì§€ í™•ì¸ ë° ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
from dateutil import parser
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
import time, json, logging, requests, os, random
from bs4 import BeautifulSoup
import boto3

logging.basicConfig(level=logging.INFO)  # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
logger = logging.getLogger(__name__)

# BUCKET_NAME = "hmg-5th-crawling-test"

BASE_URL = "https://gall.dcinside.com/board/lists/?id=car_new1"
WAIT_TIME = 2

# ì œëª©ë§Œ / ì œëª©+ë‚´ìš©
SEARCH_URL_TITLE = f"https://gall.dcinside.com/board/lists/?id=car_new1&s_type=search_subject&s_keyword="
SEARCH_URL_TITLE_AND_CONTENT = f"https://gall.dcinside.com/board/lists/?id=car_new1&s_type=search_subject_memo&s_keyword="  

def convert_date_format(date_str):
    """YY.MM.DD í˜•ì‹ì„ YY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    year, month, day = date_str.split('.')
    return f"{year}-{month}-{day}"

def is_date_in_range(date_str, start_date_str, end_date_str):
    """
    ì£¼ì–´ì§„ ë‚ ì§œ ë¬¸ìì—´ì´ íŠ¹ì • ë‚ ì§œ ë²”ìœ„ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤ (dateutil ì‚¬ìš©).

    Args:
        date_str: ê²€ì‚¬í•  ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: '23.08.17')
        start_date_str: ì‹œì‘ ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: '2023-08-16')
        end_date_str: ì¢…ë£Œ ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: '2023-11-16')

    Returns:
        bool: ë‚ ì§œê°€ ë²”ìœ„ ì•ˆì— ìˆìœ¼ë©´ True, ì•„ë‹ˆë©´ False
    """
    try:
        # dateutilì„ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        date = datetime.strptime(date_str, '%y.%m.%d')
        start_date = parser.parse(start_date_str)
        end_date = parser.parse(end_date_str)

        # ë‚ ì§œ ë²”ìœ„ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
        return start_date <= date <= end_date
    except ValueError:
        # logger.error("Error occured while parsing date")
        return False  # ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš° False ë°˜í™˜
    
class DC_crawler:
    MAX_TRY = 2
    RETRY_WAITS = 2
    post_link = [
    ]
    
    def __init__(self, s_date, e_date, car_id, car_keyword, bucket_name):
        self.start_date = s_date
        self.end_date = e_date
        self.car_id = car_id
        self.keyword = car_keyword
        self.search_url = SEARCH_URL_TITLE + car_keyword
        self.BUCKET_NAME = bucket_name
        self.s3 = boto3.client("s3")
        
    # Chrome WebDriver ì„ ì–¸, Lambda ì ìš© ì‹œ ì£¼ì„ í•„íˆ ë³´ê³  í•´ì œí•  ê²ƒ!!!!!
    def _get_driver(self,):
        # ì´ pathëŠ” ë¡œì»¬ ì‹¤í–‰ ì‹œ ì£¼ì„ì²˜ë¦¬ í•˜ì„¸ìš”.
        chrome_path = "/opt/chrome/chrome-headless-shell-mac-arm64"
        # driver_path = "/opt/chromedriver"   

        options = webdriver.ChromeOptions()
        options.binary_location = chrome_path  # Chrome ì‹¤í–‰ íŒŒì¼ ì§€ì • (ë¡œì»¬ ì‹¤í–‰ ì‹œ ì£¼ì„ ì²˜ë¦¬)
        options.add_argument("--headless=new")  # Headless ëª¨ë“œ
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--single-process")
        # options.add_argument("user-agent=Mozilla/5.0 (compatible; Daum/3.0; +http://cs.daum.net/)")
        options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:135.0) Gecko/20100101 Firefox/135.0")
        options.add_argument("--window-size=1420, 1080")
        options.add_argument('--blink-settings=imagesEnabled=false')    
        options.binary_location = "/opt/chrome/chrome-linux64/chrome"
        service = Service(executable_path="/opt/chrome-driver/chromedriver-linux64/chromedriver")
        
        driver = webdriver.Chrome(
            service=service, # ë¡œì»¬ ì‹¤í–‰ ì‹œ ì£¼ì„ ì²˜ë¦¬
            options=options) 
        return driver
    
    def get_entry_point(self, driver:webdriver.Chrome, url):
        s_date = self.start_date
        e_date = self.end_date
        
        driver.get(url)
        time.sleep(WAIT_TIME)
        #-----------------------------------------------
        # ğŸ”¹ 1. ë‚ ì§œ ê²€ìƒ‰ ì°½ì„ ì—¬ëŠ” ë²„íŠ¼ í´ë¦­
        #-----------------------------------------------
        open_date_picker = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "button.btn_grey_roundbg.btn_schmove")))
        open_date_picker.click()
        time.sleep(1)  # ê²€ìƒ‰ ì°½ì´ ëœ¨ëŠ” ì‹œê°„ ê³ ë ¤
        
        #-----------------------------------------------
        # ğŸ”¹ 2. ë‚ ì§œ ì…ë ¥ í•„ë“œ ì°¾ê¸°
        #-----------------------------------------------
        date_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "input.dayin.calendar")))
        
        #-----------------------------------------------
        # ğŸ”¹ 3. ë‚ ì§œ ì…ë ¥
        #-----------------------------------------------
        target_date = e_date  # ê²€ìƒ‰í•  ë‚ ì§œ
        # JavaScriptë¡œ ë‚ ì§œ ê°’ ë³€ê²½
        driver.execute_script("arguments[0].value = arguments[1];", date_input, target_date)
        date_input.send_keys(target_date)
        date_input.send_keys(Keys.RETURN)  # ì—”í„° ì…ë ¥

        #-----------------------------------------------
        # ğŸ”¹ 4. ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
        #-----------------------------------------------
        search_btn = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "button.btn_blue.small.fast_move_btn"))
        )
        search_btn.click()

        #-----------------------------------------------
        # ğŸ”¹ 5. ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°
        #-----------------------------------------------
        time.sleep(0.5)  # ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì— ë”°ë¼ ì¡°ì •
        
        #-----------------------------------------------
        # ğŸ”¹ 6. í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        #-----------------------------------------------
        current_page_url = driver.current_url
        return current_page_url        
        
    def crawl_post_link(self, driver:webdriver.Chrome, soup:BeautifulSoup, cur_date:str):
        """
        í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ë“¤ì˜ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        posts = soup.select("tr.ub-content.us-post")
        
        for post in posts:
            # ë‚ ì§œ ê²€ì¦
            date = post.select_one("td.gall_date").get_text(strip=True) if post.select_one("td.gall_date") else "ë‚ ì§œ ì—†ìŒ"
            if not is_date_in_range(date, self.start_date, self.end_date):
                logger.info(f"â—ï¸ Stopped by found date {date}")
                return False
            
            # ë‚ ì§œ ë„˜ì–´ê°ˆ ì‹œ ë¡œê·¸ ì‘ì„±
            if date != cur_date:
                logger.info(f"Gathering Links of {date}")
                cur_date = date
              
            gall_num = int(post.select_one("td.gall_num").get_text(strip=True))
            dc_url = "https://gall.dcinside.com"
            title_tag = post.select_one("td.gall_tit.ub-word a")
            link = dc_url + title_tag["href"] if title_tag else "ë§í¬ ì—†ìŒ"
            
            post_info = {
                "url" : link,
                "id" : gall_num,
                "date" : date
            }
            
            self.post_link.append(post_info)
        return date
    
    def page_traveler(self, driver:webdriver.Chrome, current_link:str):
        """
        í˜ì´ì§• ë°•ìŠ¤ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.
        ì‹œê°„ ì—­ìˆœìœ¼ë¡œ ìˆœíšŒí•©ë‹ˆë‹¤. 
        (í˜ì´ì§• ë°•ìŠ¤ëŠ” ì •ë°©í–¥ ìˆœíšŒ, ë³´ì´ëŠ” ê²Œì‹œê¸€ì€ ì‹œê°„ ì—­ìˆœ)
        """
        # random_sleep_time = [0.8, 0.6, 0.7, 0.5]
        cur_date = self.end_date
        i = 0
        
        while True:
            driver.get(current_link)
            time.sleep(WAIT_TIME - 1)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # is_crawl_post_success = False
            date = self.crawl_post_link(driver, soup, cur_date)
            
            if date: # ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œë¥¼ ë§Œë‚  ë•Œ ê¹Œì§€ í¬ë¡¤ë§
                # í•œ í˜ì´ì§€ë¥¼ ë‹¤ ê¸ì—ˆìœ¼ë©´...
                current_page = soup.select_one('.bottom_paging_box.iconpaging em')
                dc_url = "https://gall.dcinside.com"
                next_link = current_page.find_next_sibling('a')
                current_link = dc_url + next_link['href']
                
                # if next_link.get('class') == "search_next": 
                #     logger.info("Search next 10000 posts")
                
                time.sleep(random.randrange(500, 1000) / 1000)
                i += 1
            
                cur_date = date    
                
            else: # íŠ¹ì • ë²”ìœ„ì˜ ë‚ ì§œë¥¼ ì „ë¶€ í¬ë¡¤ë§ í–ˆë‹¤ë©´
                logger.info(f"âœ… crawling {self.start_date} ~ {self.end_date} finished")
                break
        return
    
    def get_html_of_post(self, url:str):
        """
        ê° ê²Œì‹œê¸€ì˜ html sourceë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        ê°€ì ¸ì˜¨ sourceë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        headers = {'User-Agent': "Mozilla/5.0 (compatible; Daum/3.0; +http://cs.daum.net/)"}
        for _ in range(self.MAX_TRY):
            response = requests.get(url, headers=headers)
            
            if response.status_code==200:
                html_source = response.text
                logger.info("Get link OK")
                soup = BeautifulSoup(html_source, "html_parser")
                return soup
            
            else:# í˜ì´ì§€ ì ‘ê·¼ ì¬ì‹œë„
                logger.error(f"âŒ {url} request FAILED!")
                time.sleep(self.RETRY_WAITS)
                continue
        return False
            
    def html_parser(self, driver:webdriver.Chrome, post_info:dict, parsed_post:BeautifulSoup):

        def parse_main_content(target_element):
            """
            ê²Œì‹œê¸€ ë³¸ë¬¸ í¬ë¡¤ë§
            Returns:
                ë³¸ë¬¸ ë‚´ìš©, ì¶”ì²œ ìˆ˜, ë¹„ì¶” ìˆ˜
            """
            write_div = target_element.find("div", class_="write_div")
            gaechu = int(target_element.find("p", class_="up_num font_red").get_text(strip=True))
            bichu = int(target_element.find("p", class_="down_num").get_text(strip=True))
            content = write_div.get_text(separator="\n", strip=True)  # <br>ì„ \nìœ¼ë¡œ ë³€í™˜, ê³µë°± ì œê±°
            return content, gaechu, bichu

        def parse_comments(soup):
            """
            ëŒ“ê¸€ ë° ëŒ€ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
            
            Args:
                soup (BeautifulSoup): BeautifulSoupìœ¼ë¡œ íŒŒì‹±ëœ HTML
            
            Returns:
                list[dict]: ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸
            """
            comment_list = []
            comment_ul = soup.find("ul", class_="cmt_list")
            
            if not comment_ul:
                return comment_list  # ëŒ“ê¸€ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

            for li in comment_ul.find_all("li", recursive=False):  # ìµœìƒìœ„ lië§Œ íƒìƒ‰ (ëŒ€ëŒ“ê¸€ ì œì™¸)
                # ğŸ”¹ ëŒ“ê¸€ì¸ì§€ ëŒ€ëŒ“ê¸€ì¸ì§€ êµ¬ë¶„
                is_reply = 0  # ê¸°ë³¸ì ìœ¼ë¡œ ëŒ“ê¸€(0)
                if li.find("div", class_="reply_box"):
                    is_reply = 1  # ëŒ€ëŒ“ê¸€(1)

                # ğŸ”¹ ëŒ“ê¸€ ë‚´ìš©
                content_tag = li.select_one("p.usertxt.ub-word")
                content = content_tag.get_text(strip=True) if content_tag else ""

                # ğŸ”¹ ì‘ì„± ì‹œê°„ (datetime ë³€í™˜)
                date_tag = li.select_one("span.date_time")
                created_at = datetime.strptime(date_tag.text, "%Y.%m.%d %H:%M:%S") if date_tag else None

                # ğŸ”¹ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                comment_list.append({
                    "content": content,
                    "is_reply": is_reply,
                    "created_at": created_at
                })

                # ğŸ”¹ ëŒ€ëŒ“ê¸€ íƒìƒ‰
                reply_ul = li.select_one("ul.reply_list")
                if reply_ul:
                    for reply_li in reply_ul.find_all("li", class_="ub-content"):
                        reply_content_tag = reply_li.select_one("p.usertxt.ub-word")
                        reply_content = reply_content_tag.get_text(strip=True) if reply_content_tag else ""

                        reply_date_tag = reply_li.select_one("span.date_time")
                        reply_created_at = datetime.strptime(reply_date_tag.text, "%Y.%m.%d %H:%M:%S") if reply_date_tag else None

                        comment_list.append({
                            "content": reply_content,
                            "is_reply": 1,  # ëŒ€ëŒ“ê¸€
                            "created_at": reply_created_at
                        })

            return comment_list

        def scrape_all_comment_pages(driver, soup):
            """
            ì£¼ì–´ì§„ soupì„ ê¸°ë°˜ìœ¼ë¡œ ëŒ“ê¸€ í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ëª¨ë“  ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜.
            """
            comment_count_tag = soup.find('span', class_='gall_comment')
            comment_count = int(comment_count_tag.find('a').text[len("ëŒ“ê¸€ "):]) if comment_count_tag else 0
            
            all_comments = []  # ëª¨ë“  ëŒ“ê¸€ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

            # ğŸ”¹ ì²« ë²ˆì§¸ í˜ì´ì§€ ëŒ“ê¸€ ìˆ˜ì§‘
            comments = parse_comments(soup)
            all_comments.extend(comments)
            

            # ğŸ”¹ ë‹¤ìŒ ëŒ“ê¸€ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°
            paging_box = soup.select_one("div.cmt_paging")
            if not paging_box:
                print("ëŒ“ê¸€ í˜ì´ì§€ë„¤ì´ì…˜ì´ ì—†ìŒ.")
                return all_comments

            next_page_btns = paging_box.find_all("a", href=True)

            for btn in next_page_btns:
                page_number = btn.get_text(strip=True)
                if page_number.isdigit():
                    # print(f"ì´ë™ ì¤‘: ëŒ“ê¸€ í˜ì´ì§€ {page_number}")

                    # ğŸ”¹ JavaScript ì‹¤í–‰í•˜ì—¬ ëŒ“ê¸€ í˜ì´ì§€ ì´ë™
                    driver.execute_script(btn["href"])

                    # ğŸ”¹ ìƒˆë¡œìš´ í˜ì´ì§€ HTMLì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ëŒ€ê¸°
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.cmt_paging"))
                    )

                    # ğŸ”¹ ìƒˆë¡œìš´ soup ì—…ë°ì´íŠ¸ í›„ ëŒ“ê¸€ ì¶”ê°€ ìˆ˜ì§‘
                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    comments = parse_comments(soup)
                    all_comments.extend(comments)

            return comment_count, all_comments

        post_url = post_info['url']
        post_id = post_info['id']
        created_at = parsed_post.find("span", class_="gall_date")['title']
        created_at = datetime.strptime(created_at, "%Y.%m.%d %H:%M:%S")
        title = parsed_post.find("span", class_="title_subject").get_text(strip=True)
        view_count = int(parsed_post.find("span", class_="gall_count").get_text(strip=True)[len("ì¡°íšŒ "):])
        content, up_vote, down_vote = parse_main_content(parsed_post)
        comment_count, comment_list = scrape_all_comment_pages(driver, parsed_post)
        
        parsed_finally = {
            "post_id" : post_id ,
            "post_url" : post_url,
            "title" : title,
            "content" : content,
            "created_at" : created_at,
            "view_count" : view_count,
            "upvote_count" : up_vote,
            "downvote_count" : down_vote,
            "comment_count" : comment_count,
            "comments" : comment_list
        }
                
        return parsed_finally
    
    def save_json(self, parsed_json:json, post_info:dict):
        file_path = f"extracted/{self.car_id}/{convert_date_format(post_info['date'])}/raw/dcinside/{post_info['id']}.json"
        # directory = os.path.dirname(file_path)

        
        # if not os.path.exists(directory):  # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´
        #     os.makedirs(directory)  # ë””ë ‰í† ë¦¬ ìƒì„±
        
        try:
            # with open(file_path, "w", encoding="utf-8") as file:
                # file.write(html_source)
            web_data = json.dumps(parsed_json, ensure_ascii=False, indent=4)
            
        except Exception as e:
            print(f"âŒ json dump ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")       
            
        try:
            self.s3.pub_object(
                Bucket = self.BUCKET_NAME,
                Key = file_path,
                Body = web_data,
                ContentType = "application/json"
            )     
            logger.info(f"âœ… Successfully uploaded {post_info['id']}.json to {self.BUCKET_NAME}")

        except Exception as e:
            logger.error(f"âŒ Error uploading file to S3: {e}", exc_info=True)
        
    def run_crawl(self,):
        # ë“œë¼ì´ë²„ ì„¸íŒ…
        driver=self._get_driver()
        logger.info("âœ… Driver Successfully Set.")
        
        # ê²€ìƒ‰ ê¸°ê°„ ë‚´ ê°€ì¥ ìµœì‹  ê²Œì‹œê¸€ ê²€ìƒ‰ ê²°ê³¼ ì ‘ê·¼
        end_point = self.get_entry_point(driver, url=self.search_url)
        logger.info("âœ… Successfully accessed to init date")
        
        # ì ‘ê·¼ ìœ„ì¹˜ë¡œë¶€í„° ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘
        self.page_traveler(driver, end_point)
        
        # ìˆ˜ì§‘ëœ ë§í¬ë¥¼ ë°©ë¬¸í•˜ë©° html ì†ŒìŠ¤ ì €ì¥
        for i, post in enumerate(self.post_link):
            # print(f"Progressing... [{i+1} / {len(self.post_link)}]")
            
            # random_sleep_time = [0.8, 0.6, 0.7, 0.5]
            parsed_source = self.get_html_of_post(post['url'])
            res_json = self.html_parser(driver, post, parsed_source)
            
            logger.info(f"Saving...[{i+1} / {len(self.post_link)}]")
            self.save_json(res_json, post)
                
            time.sleep(1 + random.randrange(500, 1000) / 1000)
                    

    
def lambda_handler(event, context):
    BUCKET_NAME = event.get('bucket_name')
    car_keyword = event.get('car_keyword')
    # car = {'ì‚°íƒ€í˜': [ # ì°¨ì¢…
    #             'ì‚°íƒ€í˜', # í•´ë‹¹ ì°¨ì¢…ì˜ ì´ëª…
    #             'ì‹¼íƒ€í˜']
    #     }   
  
    s_date="2023-08-16"
    e_date="2023-11-16"
    
    logger.info(f"âœ… Initiating Crawler : {s_date} ~ {e_date}")
    
    # car_keywordëŠ” lambda_handlerì—ì„œ eventë¡œ ì²˜ë¦¬í•˜ê²Œ í•  ê²ƒ
    crawler = DC_crawler(s_date, e_date, car_id="santafe", car_keyword=car_keyword, bucket_name=BUCKET_NAME)
    
    logger.info("Running crawler")
    crawler.run_crawl()
    logger.info("âœ… Crawling Finished")
    